{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CENSNET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-tn2ZBJ4j5N",
        "outputId": "3e9c9cfe-228b-451c-f0b1-5150df166ca8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.63.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.7.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.5)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from datetime import date, timedelta\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "import time"
      ],
      "metadata": {
        "id": "sR1h9EQ34J4i"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning import LightningModule\n"
      ],
      "metadata": {
        "id": "ZnPl2zM7m6Sb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7f56BAgv4k1n"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "YN9fL3Io-asK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Data/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJeqGPsu-Abq",
        "outputId": "3866f624-e0a9-4dd3-d03b-69761524f560"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = 'la_weather_with_pm_per_day.csv'\n",
        "weather_data = pd.read_csv(path+file1)\n",
        "file2 = 'metadata_with_station.csv'\n",
        "metadata = pd.read_csv(path+file2)\n",
        "metadata = metadata[metadata['location'] == 'Los Angeles (SoCAB)'].reset_index(drop = True)"
      ],
      "metadata": {
        "id": "ORtbVknp9x0G"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_to_numpy(weather_data, edge_names, node_names, station_ids, date_range): \n",
        "\n",
        "    graph_node_features = np.empty((len(date_range), len(station_ids), len(node_names)))\n",
        "    graph_edge_features = np.empty((len(date_range), len(station_ids), len(edge_names)))\n",
        "    graph_labels = np.empty((len(date_range), len(station_ids)))\n",
        "\n",
        "    for day_idx in range(len(date_range)): \n",
        "        for station_idx in range(len(station_ids)): \n",
        "                d = date_range[day_idx]             #get date from index\n",
        "                station = station_ids[station_idx]     #get station number from index\n",
        "                vals = weather_data[weather_data['DATE'] == d]  #get data date using date\n",
        "                vals = vals[vals['STATION'] == station]         #get data of station on date\n",
        "                pm = vals['pm25'].values\n",
        "                edge = vals[edge_names]\n",
        "                edge_vals = np.array(edge.values.tolist()).flatten()  #edge feature as array\n",
        "                node_vals = vals[node_names]\n",
        "                node_vals = np.array(node_vals.values.tolist()).flatten() #node features as array\n",
        "                if len(node_vals) == 0:                           #if there is no weather data on date, set to all zeros\n",
        "                    node_vals = np.zeros(len(node_names))\n",
        "                graph_node_features[day_idx, station_idx] = node_vals \n",
        "                if len(pm) == 0:     #if no pm label on date set to 0\n",
        "                    pm = np.zeros(1)\n",
        "                graph_labels[day_idx, station_idx] = pm\n",
        "                if len(edge_vals) == 0:  #if no edge feature, set to 0\n",
        "                    edge_vals = np.zeros(len(edge_names))\n",
        "                graph_edge_features[day_idx, station_idx] = edge_vals\n",
        "\n",
        "    return graph_node_features, graph_edge_features, graph_labels\n",
        "\n",
        "def generate_nodes(metadata, stations):\n",
        "    stations.sort()\n",
        "    nodes = OrderedDict()\n",
        "    for id in stations:\n",
        "        row = metadata[metadata['STATION'] == id]\n",
        "        lat = row['Latitudes'].values[0]\n",
        "        lon = row['Longitudes'].values[0]\n",
        "        # elev = row['Elevetation']\n",
        "        nodes.update({id:{'Latitude':lat,'Longitude':lon}})\n",
        "    return nodes\n",
        "\n",
        "def geo_distance(first_node, second_node):\n",
        "        '''Haversine formula for geodesic distance'''\n",
        "        lat1, long1 = first_node\n",
        "        lat2, long2 = second_node\n",
        "        R = 6371e3 #m\n",
        "        lat1 = lat1 * np.pi/180; #rad\n",
        "        lat2 = lat2 * np.pi/180; #rad\n",
        "        delta_lat = (lat2 - lat1) * np.pi/180;\n",
        "        delta_long = (long1 - long2) * np.pi/180;\n",
        "        a = (np.sin(delta_lat/2))**2 + np.cos(lat1) * np.cos(lat2) * np.sin(delta_long/2) * np.sin(delta_long/2)\n",
        "        c = 2 * np.arctan2(a**0.5, (1-a)**0.5)\n",
        "        d = R * c #m\n",
        "        return d\n",
        "\n",
        "def generate_node_distances(coordinates):\n",
        "    distance_matrix = np.zeros((len(coordinates), len(coordinates)))\n",
        "    for i in range(len(coordinates)): \n",
        "        coord1 = coordinates[i]\n",
        "        for j in range(len(coordinates)): \n",
        "            coord2 = coordinates[j]\n",
        "            distance = geo_distance(coord1, coord2)\n",
        "            distance_matrix[i][j] = distance\n",
        "    return distance_matrix\n",
        "\n",
        "def sparse_adjacency(adj): \n",
        "    \"\"\"Converts a dense adjacency matrix to a sparse adjacency matrix defined\n",
        "    by edge indices and edge attributes.\n",
        "    \"\"\"\n",
        "    assert adj.dim() >= 2 and adj.dim() <= 3\n",
        "    assert adj.size(-1) == adj.size(-2)\n",
        "\n",
        "    index = adj.nonzero(as_tuple=True)\n",
        "    edge_attr = adj[index]\n",
        "\n",
        "    if len(index) == 3:\n",
        "        batch = index[0] * adj.size(-1)\n",
        "        index = (batch + index[1], batch + index[2])\n",
        "\n",
        "    return torch.stack(index, dim=0), edge_attr\n",
        "\n",
        "def add_self_loops_to_sparse_adj(edge_idx, n):\n",
        "        source_w_self_loop = np.append(edge_idx[0], [i for i in range(n)])    #add self loops\n",
        "        target_w_self_loop = np.append(edge_idx[1], [i for i in range(n)])\n",
        "        edge_idx = np.array([source_w_self_loop, target_w_self_loop])\n",
        "        order = edge_idx[0].argsort()\n",
        "        edge_idx[0].sort()\n",
        "        edge_idx[1] = edge_idx[1][order] \n",
        "        return(edge_idx) \n",
        "\n",
        "class Graph():\n",
        "    def __init__(self,\n",
        "                 graph_metadata,\n",
        "                 stations,\n",
        "                 edge_data, \n",
        "                 distance_threshold):\n",
        "        self.graph_metadata = graph_metadata[graph_metadata['STATION'].isin(stations)]\n",
        "        self.distance_threshold = distance_threshold\n",
        "        self.nodes = generate_nodes(graph_metadata, stations)\n",
        "        self.size = len(self.nodes)\n",
        "        self.edge_data = edge_data\n",
        "        self.edge_index, self.edge_attr = self.generate_edges()\n",
        "        self.adjacency_matrix = self.get_adjacency_matrix()\n",
        "        self.edge_adjacency = self.get_edge_adjacency()\n",
        "        self.transformation_matrix = self.get_transformation_matrix()\n",
        "\n",
        "    def generate_edges(self):\n",
        "        nodes = self.nodes            \n",
        "        node_list = list(self.nodes)        #get list of node ids\n",
        "        coordinates = list(zip(self.graph_metadata['Latitudes'], self.graph_metadata['Longitudes'])) \n",
        "        distance_matrix = generate_node_distances(coordinates)  #square matrix of distance to all other nodes\n",
        "        adj_matrix = np.zeros([self.size, self.size])\n",
        "        adj_matrix[distance_matrix < self.distance_threshold] = 1 #in adj matrix, set entry to 1 if distance between nodes below threshold\n",
        "\n",
        "        distance_matrix = distance_matrix * adj_matrix\n",
        "        mean_distance = np.mean(distance_matrix)\n",
        "        std_distance = np.std(distance_matrix)\n",
        "\n",
        "        edge_idx, edge_dist = sparse_adjacency(torch.tensor(distance_matrix)) #edge_idx : shape (2 * number of connected nodes (dis < threshold)) \n",
        "        edge_idx, edge_dist = edge_idx.numpy(), edge_dist.numpy()             #edge_dist: same shape as above, distance values between those node indices\n",
        "        edge_idx = add_self_loops_to_sparse_adj(edge_idx, len(node_list))\n",
        "        edge_idx = add_self_loops_to_sparse_adj(edge_idx, len(node_list))\n",
        "\n",
        "        windx, windy, dx, dy = [],[],[],[]\n",
        "        for i in range(edge_idx.shape[1]):\n",
        "            #get index of non-zero edges\n",
        "            source_idx = edge_idx[0, i]\n",
        "            dest_idx = edge_idx[1, i]\n",
        "            #get lat lon for the nodes at ends of non zero edge\n",
        "            key0 = node_list[source_idx]\n",
        "            lat0 = nodes[key0]['Latitude']\n",
        "            long0 = nodes[key0]['Longitude']\n",
        "            key1 = node_list[dest_idx]\n",
        "            lat1 = nodes[key1]['Latitude']\n",
        "            long1 = nodes[key1]['Longitude']\n",
        "            distance_x = geo_distance((lat0, 0), (lat1, 0))\n",
        "            distance_y = geo_distance((0, long1), (0, long1))\n",
        "            edge_vect_x = distance_x \n",
        "            #get wind along x and y vectors from both source and edge\n",
        "            wind_source_x = self.edge_data[:, source_idx, 0]\n",
        "            wind_dest_x = self.edge_data[:, dest_idx, 0]\n",
        "            wind_source_y = self.edge_data[:, source_idx, 1]\n",
        "            wind_dest_y = self.edge_data[:, dest_idx, 1]\n",
        "            #average source and edge wind components to get net wind \n",
        "            wind_x = (wind_source_x + wind_dest_x)/2\n",
        "            wind_y = (wind_source_y + wind_dest_y)/2\n",
        "            mean_wind_x, mean_wind_y = np.mean(wind_x), np.mean(wind_y)\n",
        "            std_wind_x, std_wind_y = np.std(wind_x), np.std(wind_y)\n",
        "            wind_x = (wind_x - mean_wind_x) / std_wind_x\n",
        "            wind_y = (wind_y - mean_wind_y) / std_wind_y\n",
        "            distance_x = (distance_x - mean_distance) / std_distance\n",
        "            distance_x = (distance_y - mean_distance) / std_distance\n",
        "\n",
        "            # edge_vectors.append(np.array([distance_x, distance_y, wind_x, wind_y]))\n",
        "            windx.append(wind_x), windy.append(wind_y)\n",
        "            dx.append(distance_x), dy.append(distance_y)\n",
        "        windx = np.stack(windx).transpose()\n",
        "        windy = np.stack(windy).transpose()\n",
        "        dx = np.tile(np.stack(dx), [windx.shape[0],1])\n",
        "        dy = np.tile(np.stack(dy), [windx.shape[0],1])\n",
        "        edge_vectors = np.array([windx, windy, dx, dy]).transpose(1,2,0)\n",
        "        return edge_idx, edge_vectors\n",
        "\n",
        "    def edge_list_to_adj(self):\n",
        "        adj = np.identity(self.edge_index.size)\n",
        "        for k,(i,j) in enumerate(zip(self.edge_index[0], self.edge_index[1])):\n",
        "            adj[i,j] = self.edge_attr[j]\n",
        "        return adj\n",
        "\n",
        "    def edge_list_sequence_to_adj(self):\n",
        "        adjacencies = []\n",
        "        for i in range(self.edge_attr.shape[0]):\n",
        "            adj = np.identity(self.size)\n",
        "            for k,(i,j) in enumerate(zip(self.edge_index[0], self.edge_index[1])):\n",
        "                adj[i,j] = self.edge_attr[i][k]\n",
        "            adjacencies.append(adj)\n",
        "        return np.array(adjacencies)\n",
        "\n",
        "    def get_edge_adjacency(self):\n",
        "        adj_e = np.zeros([self.edge_index.shape[1],self.edge_index.shape[1]])\n",
        "        for i,s in enumerate(self.edge_index[0]):\n",
        "            t = self.edge_index[1][i]\n",
        "            adj_e[i,t] = 1\n",
        "        assert adj_e.all() == adj_e.transpose().all()\n",
        "        return adj_e\n",
        "\n",
        "    def get_adjacency_matrix(self):\n",
        "        nodes = self.nodes            \n",
        "        node_list = list(self.nodes)        #get list of node ids\n",
        "        coordinates = list(zip(self.graph_metadata['Latitudes'], self.graph_metadata['Longitudes']))  #zip latitude and longitude of all nodes\n",
        "        distance_matrix = generate_node_distances(coordinates)  #square matrix of distance to all other nodes\n",
        "\n",
        "        adj = np.identity(self.size)\n",
        "        adj[distance_matrix < self.distance_threshold] = 1 #in adj matrix, set entry to 1 if distance between nodes below threshold\n",
        "        assert adj.all() == adj.transpose().all()\n",
        "        return adj\n",
        "\n",
        "    def get_transformation_matrix(self):\n",
        "        T = np.zeros([self.size, self.edge_adjacency.shape[0]])\n",
        "        for i,source in enumerate(self.edge_index[0]):\n",
        "            for j,t in enumerate(self.edge_index[1]): \n",
        "                if self.edge_index[0][j]  == source:        \n",
        "                    T[source, t] = 1      \n",
        "        assert T.all() == T.transpose().all()\n",
        "        return T\n",
        "\n"
      ],
      "metadata": {
        "id": "xmDbPf5L-xCK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stations_2018_02 = weather_data[weather_data['DATE'] == '2018-02-01']['STATION'].unique() \n",
        "stations_2018_06 = weather_data[weather_data['DATE'] == '2018-06-08']['STATION'].unique() \n",
        "stations_2020_01 = weather_data[weather_data['DATE'] == '2020-01-03']['STATION'].unique() \n",
        "stations_2020_12 = weather_data[weather_data['DATE'] == '2020-12-31']['STATION'].unique() \n",
        "\n",
        "\n",
        "stations = [value for value in stations_2018_02 if value in stations_2018_06]"
      ],
      "metadata": {
        "id": "CYzT9IxSN7xk"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = date(2018,2,1)\n",
        "end = date(2021,6,8)\n",
        "date_range = pd.date_range(start,end-timedelta(days=1))\n",
        "date_range = [str(x)[:10] for x in date_range]\n",
        "node_cols = ['ceiling', 'visibility', 'dew', 'precipitation_duration', 'precipitation_depth', 'mean_aod','min_aod','max_aod']\n",
        "edge_cols = ['wind_x', 'wind_y']\n",
        "stations.sort()\n",
        "weather_data = weather_data[weather_data['STATION'].isin(stations)]\n",
        "weather_data = weather_data[weather_data['DATE'].isin(date_range)]\n",
        "weather_data\n",
        "weather_data = weather_data[['STATION','DATE','pm25']+edge_cols+node_cols]\n",
        "weather_data = weather_data.fillna(weather_data.mean())\n",
        "\n",
        "\n",
        "graph_node_features, graph_edge_features, graph_labels = data_to_numpy(weather_data, edge_cols, node_cols, stations, date_range)\n",
        "print(graph_node_features.shape)\n",
        "print(graph_edge_features.shape)\n",
        "print(graph_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j98J3B37-xWH",
        "outputId": "a4864c88-9136-4937-ec0c-0c18566db1eb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1223, 9, 8)\n",
            "(1223, 9, 2)\n",
            "(1223, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Graph(metadata, stations, graph_edge_features, distance_threshold = 30e3)\n"
      ],
      "metadata": {
        "id": "moZcNZEc-xdV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.adjacency_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq8ZcnqQ5xAX",
        "outputId": "32f82ff3-3563-4ec7-ca05-09fd0593831b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sliding_timeframe(vect, sequence_len):\n",
        "    vect_len = vect.shape[0]\n",
        "    timeframes = []\n",
        "    for i in range(sequence_len, vect_len):\n",
        "        timeframe = vect[i - sequence_len : i]\n",
        "        timeframes.append(timeframe)\n",
        "    timeframes = np.stack(timeframes, axis = 0)\n",
        "    return timeframes\n",
        "\n",
        "def normalize(vect, mean, std):\n",
        "    norm_v = (vect - mean) / std\n",
        "    return norm_v\n",
        "\n",
        "\n",
        "\n",
        "class WeatherData(Dataset):\n",
        "    def __init__(self, \n",
        "                 labels, \n",
        "                 edge_features,   \n",
        "                 node_features,\n",
        "                 historical_len, \n",
        "                 prediction_len, \n",
        "                 sequence = True):\n",
        "        \n",
        "        self.historical_len = historical_len\n",
        "        self.prediction_len = prediction_len\n",
        "        self.sequence_len = historical_len + prediction_len\n",
        "\n",
        "        self.edge_features = edge_features\n",
        "\n",
        "        label_mean = labels.mean(axis = (0,1))\n",
        "        label_sdev = labels.std(axis = (0,1))\n",
        "        self.labels = normalize(labels, label_mean, label_sdev)\n",
        "\n",
        "        feature_mean = node_features.mean()\n",
        "        feature_sdev = node_features.std()\n",
        "        self.features = normalize(node_features, feature_mean, feature_sdev)\n",
        "\n",
        "        if sequence: \n",
        "            self.features = get_sliding_timeframe(self.features, self.sequence_len)\n",
        "            self.edge_features = get_sliding_timeframe(self.edge_features, self.sequence_len)\n",
        "            self.labels = get_sliding_timeframe(self.labels, self.sequence_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.features[idx], self.edge_features[idx], self.labels[idx]\n",
        "        return data"
      ],
      "metadata": {
        "id": "j4WVYoxN0ScU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "u6-9LGW936CI"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features_v, out_features_v, in_features_e, out_features_e, bias=True, node_layer=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features_e = in_features_e\n",
        "        self.out_features_e = out_features_e\n",
        "        self.in_features_v = in_features_v\n",
        "        self.out_features_v = out_features_v\n",
        "\n",
        "        if node_layer:\n",
        "            self.node_layer = True\n",
        "            self.weight = Parameter(torch.FloatTensor(in_features_v, out_features_v))\n",
        "            self.p = Parameter(torch.from_numpy(np.random.normal(size=(1, in_features_e))).float())\n",
        "            if bias:\n",
        "                self.bias = Parameter(torch.FloatTensor(out_features_v))\n",
        "            else:\n",
        "                self.register_parameter('bias', None)\n",
        "        else:\n",
        "            self.node_layer = False\n",
        "            self.weight = Parameter(torch.FloatTensor(in_features_e, out_features_e))\n",
        "            self.p = Parameter(torch.from_numpy(np.random.normal(size=(1, in_features_v))).float())\n",
        "            if bias:\n",
        "                self.bias = Parameter(torch.FloatTensor(out_features_e))\n",
        "            else:\n",
        "                self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, H_v, H_e, adj_e, adj_v, T):\n",
        "        if self.node_layer:\n",
        "            multiplier1 = torch.mm(T, torch.diag((H_e @ self.p.t()).t()[0])) @ T.t()\n",
        "            mask1 = torch.eye(multiplier1.shape[0])\n",
        "            M1 = mask1 * torch.ones(multiplier1.shape[0]) + (1. - mask1)*multiplier1\n",
        "            adjusted_A = torch.mul(M1, adj_v)\n",
        "            '''\n",
        "            print(\"adjusted_A is \", adjusted_A)\n",
        "            normalized_adjusted_A = adjusted_A / adjusted_A.max(0, keepdim=True)[0]\n",
        "            print(\"normalized adjusted A is \", normalized_adjusted_A)\n",
        "            '''\n",
        "            # to avoid missing feature's influence, we don't normalize the A\n",
        "            output = torch.mm(adjusted_A, torch.mm(H_v, self.weight))\n",
        "            if self.bias is not None:\n",
        "                ret = output + self.bias\n",
        "            return ret, H_e\n",
        "\n",
        "        else:\n",
        "            multiplier2 = torch.spmm(T.t(), torch.diag((H_v @ self.p.t()).t()[0])) @ T\n",
        "            mask2 = torch.eye(multiplier2.shape[0])\n",
        "            M3 = mask2 * torch.ones(multiplier2.shape[0]) + (1. - mask2)*multiplier2\n",
        "            adjusted_A = torch.mul(M3, adj_e)\n",
        "            normalized_adjusted_A = adjusted_A / adjusted_A.max(0, keepdim=True)[0]\n",
        "            output = torch.mm(adjusted_A.float(), torch.mm(H_e, self.weight).float())\n",
        "            if self.bias is not None:\n",
        "                ret = output + self.bias\n",
        "            else: \n",
        "                ret = output\n",
        "            return H_v, ret\n",
        "\n",
        "\n",
        "        \n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat_v, nfeat_e, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        " \n",
        "        self.gc1 = GraphConvolution(nfeat_v, nhid, nfeat_e, nfeat_e, node_layer=True).to(device)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid, nfeat_e, nfeat_e, node_layer=False).to(device)\n",
        "        self.gc3 = GraphConvolution(nhid, nclass, nfeat_e, nfeat_e, node_layer=True).to(device)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, X, Z, adj_e, adj_v, T):\n",
        "        # print x \n",
        "        gc1 = self.gc1(X, Z, adj_e, adj_v, T)\n",
        "        X, Z = F.relu(gc1[0]), F.relu(gc1[1])\n",
        "        \n",
        "        X = F.dropout(X, self.dropout, training=self.training)\n",
        "        Z = F.dropout(Z, self.dropout, training=self.training)\n",
        "\n",
        "        gc2 = self.gc2(X, Z, adj_e, adj_v, T)\n",
        "        X, Z = F.relu(gc2[0]), F.relu(gc2[1])\n",
        "\n",
        "\n",
        "        X = F.dropout(X, self.dropout, training=self.training)\n",
        "        Z = F.dropout(Z, self.dropout, training=self.training)\n",
        "\n",
        "        X, Z = self.gc3(X, Z, adj_e, adj_v,T)\n",
        "        return F.log_softmax(X, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = torch.from_numpy(graph_node_features)\n",
        "edge_features = torch.from_numpy(graph.edge_attr)\n",
        "labels = torch.from_numpy(graph_labels)\n",
        "\n",
        "dataset = WeatherData(labels = labels,\n",
        "                      edge_features = edge_features,\n",
        "                      node_features = node_features,\n",
        "                      historical_len = 5,\n",
        "                      prediction_len = 2, \n",
        "                      sequence = False)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 1)\n",
        "for data in dataloader:\n",
        "    node_features, edge_features, labels = data\n",
        "    # graph_data = Data(node_features, edge_index, edge_features, labels)\n",
        "    break\n",
        "    "
      ],
      "metadata": {
        "id": "pVnleha80WFH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "model = GCN(nfeat_v = 8, nfeat_e = 4, nhid = 4, nclass = 1, dropout = 0.6)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "node_features = node_features.to(device).float().squeeze()\n",
        "edge_features = edge_features.to(device).float().squeeze()\n",
        "adj_e = adj_e = torch.from_numpy(graph.edge_adjacency)\n",
        "adj_e.to(device).float().squeeze()\n",
        "adj = torch.from_numpy(graph.adjacency_matrix)\n",
        "adj = adj.to(device).float().squeeze()\n",
        "T = torch.from_numpy(graph.transformation_matrix)\n",
        "T = T.to(device).float().squeeze()\n"
      ],
      "metadata": {
        "id": "CSgcxqYe0VGF"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjgPOw_7s5i5",
        "outputId": "e3445961-4810-4114-ab49-c6762de36aec"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0., grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCp6qQGXs9gY",
        "outputId": "2cf8bcf4-6f66-412b-a008-c04131f20996"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "idx_train = [0,1,4,6,7,8]\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(node_features, edge_features, adj_e, adj, T)\n",
        "    train_labels = labels[0][idx_train]\n",
        "    train_output = output[idx_train].squeeze()\n",
        "    loss_train = criteria(train_output, train_labels)\n",
        "\n",
        "    print(output)\n",
        "\n",
        "    loss_train.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "YlqV737d5vtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1254f356-1eaa-4915-c920-e737f0c51bca"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n",
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output[idx_train]"
      ],
      "metadata": {
        "id": "r7lFracOopG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj_e"
      ],
      "metadata": {
        "id": "DrOwOBUF4tTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weather_data = weather_data.fillna(weather_data.mean())"
      ],
      "metadata": {
        "id": "P9maLBTKwXqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3bfjh9mMx_lK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}