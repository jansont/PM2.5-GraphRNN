{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate grids for LA region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_geopanda(df):\n",
    "  ''' df must have a 'wkt' column'''\n",
    "  df['geometry'] = gpd.GeoSeries.from_wkt(df['wkt']) #convert the wkt column to a polygon object\n",
    "  df = df.drop(labels=['wkt'], axis=1)\n",
    "  df = gpd.GeoDataFrame(df, geometry='geometry')     # set the wkt column to be the geometry column\n",
    "\n",
    "  #idk what this is but it fixed the areas (they were like 0.001...)\n",
    "  # df.crs = 'epsg:4326'\n",
    "  # df = df.to_crs(\"+proj=cea +lat_0=35.68250088833567 +lon_0=139.7671 +units=m\")\n",
    "\n",
    "  df['area'] = df.area /1000000                                   #it looks like its computing areas in meters, so divide by 1mil\n",
    "  \n",
    "  df['boundary'] = df.boundary                        #compute the boundaries of each grid\n",
    "  df['centroid'] = df.centroid                         #compute centroids\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_polygons = convert_to_geopanda(pd.read_csv('/mnt/d/airdata/la_metadata_with_station_v2.csv'))\n",
    "\n",
    "grid_metadata = pd.read_csv(\"/mnt/d/airdata/grid_metadata.csv\").set_index('grid_id')\n",
    "grid_metadata = convert_to_geopanda(grid_metadata)\n",
    "grid_to_color = create_colors(grid_metadata, 'hsv')\n",
    "g = pd.DataFrame.from_dict(grid_to_color).transpose()\n",
    "g['color'] = g.values.tolist()\n",
    "grid_metadata = grid_metadata.join(g['color'].map(lambda x: tuple(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-118.54018764,   33.66484012, -116.87830437,   34.18660804])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_polygons['centroid'].total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_h(df):\n",
    "    \n",
    "\n",
    "    minx, miny, maxx, maxy = df['geometry'].bounds\n",
    "\n",
    "    width = maxx - minx\n",
    "    height = maxy - miny\n",
    "    return width, height\n",
    "\n",
    "\n",
    "location_polygons[['width', 'height']] = location_polygons.apply(w_h, axis=1, result_type ='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04491576420599367, 3.005976711873475e-14)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_polygons['width'].mean(), location_polygons['width'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.037240274823499404, 6.89354812462686e-05)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_polygons['height'].mean(), location_polygons['height'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "minx, miny, maxx, maxy = location_polygons['centroid'].total_bounds\n",
    "box_y = location_polygons['height'].mean()\n",
    "box_x = location_polygons['width'].mean()\n",
    "\n",
    "xs = np.arange(minx, maxx, box_x)\n",
    "ys = np.arange(miny, maxy, box_y)\n",
    "\n",
    "centers = []\n",
    "for x in xs:\n",
    "    for y in ys:\n",
    "        p1 = Point(x+box_x, y+box_y)\n",
    "        p2 = Point(x-box_x, y+box_y)\n",
    "        p3 = Point(x+box_x, y-box_y)\n",
    "        p4 = Point(x-box_x, y-box_y)\n",
    "        centers.append(Polygon([p1, p2, p3, p4, p1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = [i.centroid.x for i in centers] \n",
    "long = [i.centroid.y for i in centers] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'wkt': centers, 'Latitudes': lat, 'Longitudes': long})\n",
    "df.to_csv('la_generated_grid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract AOD values\n",
    "\n",
    "\n",
    "Note, we need to restart the kernel everytime because of a bug in multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/weather-forecast/.venv/lib/python3.8/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/mnt/d/weather-forecast/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from cloudpathlib import S3Client, S3Path\n",
    "from pqdm.processes import pqdm\n",
    "# from pqdm.threads import pqdm \n",
    "from pyproj import CRS, Proj\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyproj\n",
    "from pyhdf.SD import SD, SDC, SDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/mnt/d/airdata\")\n",
    "RAW = DATA_PATH / \"raw\"\n",
    "INTERIM = DATA_PATH / \"interim\"\n",
    "pm_md = pd.read_csv(\n",
    "    INTERIM / \"pm25_satellite_metadata.csv\",\n",
    "        parse_dates=[\"time_start\", \"time_end\"],\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "grid_md = pd.read_csv(\n",
    "    DATA_PATH / \"la_generated_grid.csv\",\n",
    ")\n",
    "grid_md['location'] = \"Los Angeles (SoCAB)\"\n",
    "grid_md['grid_id'] = grid_md['wkt'].apply(lambda x: hash(str(x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_arrays(\n",
    "    xv: Union[np.array, float],\n",
    "    yv: Union[np.array, float],\n",
    "    crs_from: CRS,\n",
    "    crs_to: CRS\n",
    "):\n",
    "    \"\"\"Transform points or arrays from one CRS to another CRS.\n",
    "    \n",
    "    Args:\n",
    "        xv (np.array or float): x (longitude) coordinates or value.\n",
    "        yv (np.array or float): y (latitude) coordinates or value.\n",
    "        crs_from (CRS): source coordinate reference system.\n",
    "        crs_to (CRS): destination coordinate reference system.\n",
    "    \n",
    "    Returns:\n",
    "        lon, lat (tuple): x coordinate(s), y coordinate(s)\n",
    "    \"\"\"\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        crs_from,\n",
    "        crs_to,\n",
    "        always_xy=True,\n",
    "    )\n",
    "    lon, lat = transformer.transform(xv, yv)\n",
    "    return lon, lat\n",
    "def calculate_features(\n",
    "    feature_df: gpd.GeoDataFrame,\n",
    "):\n",
    "    \"\"\"Given processed AOD data and training labels (train) or \n",
    "    submission format (test), return a feature GeoDataFrame that contains\n",
    "    features for mean, max, and min AOD.\n",
    "    \n",
    "    Args:\n",
    "        feature_df (gpd.GeoDataFrame): GeoDataFrame that contains\n",
    "            Points and associated values.\n",
    "        label_df (pd.DataFrame): training labels (train) or\n",
    "            submission format (test).\n",
    "        stage (str): \"train\" or \"test\".\n",
    "    \n",
    "    Returns:\n",
    "        full_data (gpd.GeoDataFrame): GeoDataFrame that contains `mean_aod`,\n",
    "            `max_aod`, and `min_aod` for each grid cell and datetime.   \n",
    "    \"\"\"\n",
    "    # Add `day` column to `feature_df` and `label_df`\n",
    "    feature_df[\"datetime\"] = pd.to_datetime(\n",
    "        feature_df.granule_id.str.split(\"_\", expand=True)[0],\n",
    "        format=\"%Y%m%dT%H:%M:%S\",\n",
    "        utc=True\n",
    "    )\n",
    "    feature_df[\"day\"] = feature_df.datetime.dt.date\n",
    "    # label_df[\"day\"] = label_df.datetime.dt.date\n",
    "\n",
    "    # Calculate average AOD per day/grid cell for which we have feature data\n",
    "    feature_df['geometry'] = feature_df['geometry'].apply(lambda x: str(x))\n",
    "    avg_aod_day = feature_df.groupby([\"day\", \"geometry\"]).agg(\n",
    "        {\"value\": [\"mean\", \"min\", \"max\"]}\n",
    "    )\n",
    "    avg_aod_day.columns = [\"mean_aod\", \"min_aod\", \"max_aod\"]\n",
    "    avg_aod_day = avg_aod_day.reset_index()\n",
    "    return avg_aod_day\n",
    "    # # Join labels/submission format with feature data\n",
    "    # how = \"inner\" if stage == \"train\" else \"left\"\n",
    "    # full_data = pd.merge(\n",
    "    #     label_df,\n",
    "    #     avg_aod_day,\n",
    "    #     how=how,\n",
    "    #     left_on=[\"day\", \"grid_id\"],\n",
    "    #     right_on=[\"day\", \"grid_id\"]\n",
    "    # )\n",
    "    # return full_data\n",
    "\n",
    "def create_meshgrid(alignment_dict: Dict, shape: List[int]):\n",
    "    \"\"\"Given an image shape, create a meshgrid of points\n",
    "    between bounding coordinates.\n",
    "    \n",
    "    Args:\n",
    "        alignment_dict (Dict): dictionary containing, at a minimum,\n",
    "            `upper_left` (tuple), `lower_right` (tuple), `crs` (str),\n",
    "            and `crs_params` (tuple).\n",
    "        shape (List[int]): dataset shape as a list of\n",
    "            [orbits, height, width].\n",
    "    \n",
    "    Returns:\n",
    "        xv (np.array): x (longitude) coordinates.\n",
    "        yv (np.array): y (latitude) coordinates.\n",
    "    \"\"\"\n",
    "    # Determine grid bounds using two coordinates\n",
    "    x0, y0 = alignment_dict[\"upper_left\"]\n",
    "    x1, y1 = alignment_dict[\"lower_right\"]\n",
    "    \n",
    "    # Interpolate points between corners, inclusive of bounds\n",
    "    x = np.linspace(x0, x1, shape[2], endpoint=True)\n",
    "    y = np.linspace(y0, y1, shape[1], endpoint=True)\n",
    "    \n",
    "    # Return two 2D arrays representing X & Y coordinates of all points\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    return xv, yv\n",
    "\n",
    "\n",
    "def calibrate_data(dataset: SDS, shape: List[int], calibration_dict: Dict):\n",
    "    \"\"\"Given a MAIAC dataset and calibration parameters, return a masked\n",
    "    array of calibrated data.\n",
    "    \n",
    "    Args:\n",
    "        dataset (SDS): dataset in SDS format (e.g. blue band AOD).\n",
    "        shape (List[int]): dataset shape as a list of [orbits, height, width].\n",
    "        calibration_dict (Dict): dictionary containing, at a minimum,\n",
    "            `valid_range` (list or tuple), `_FillValue` (int or float),\n",
    "            `add_offset` (float), and `scale_factor` (float).\n",
    "    \n",
    "    Returns:\n",
    "        corrected_AOD (np.ma.MaskedArray): masked array of calibrated data\n",
    "            with a fill value of nan.\n",
    "    \"\"\"\n",
    "    corrected_AOD = np.ma.empty(shape, dtype=np.double)\n",
    "    for orbit in range(shape[0]):\n",
    "        data = dataset[orbit, :, :].astype(np.double)\n",
    "        invalid_condition = (\n",
    "            (data < calibration_dict[\"valid_range\"][0]) |\n",
    "            (data > calibration_dict[\"valid_range\"][1]) |\n",
    "            (data == calibration_dict[\"_FillValue\"])\n",
    "        )\n",
    "        data[invalid_condition] = np.nan\n",
    "        data = (\n",
    "            (data - calibration_dict[\"add_offset\"]) *\n",
    "            calibration_dict[\"scale_factor\"]\n",
    "        )\n",
    "        data = np.ma.masked_array(data, np.isnan(data))\n",
    "        corrected_AOD[orbit, : :] = data\n",
    "    corrected_AOD.fill_value = np.nan\n",
    "    return corrected_AOD\n",
    "\n",
    "\n",
    "def convert_array_to_df(\n",
    "    corrected_arr: np.ma.MaskedArray,\n",
    "    lat:np.ndarray,\n",
    "    lon: np.ndarray,\n",
    "    granule_id: str,\n",
    "    crs: CRS,\n",
    "    total_bounds: np.ndarray = None\n",
    "):\n",
    "    \"\"\"Align data values with latitude and longitude coordinates\n",
    "    and return a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "        corrected_arr (np.ma.MaskedArray): data values for each pixel.\n",
    "        lat (np.ndarray): latitude for each pixel.\n",
    "        lon (np.ndarray): longitude for each pixel.\n",
    "        granule_id (str): granule name.\n",
    "        crs (CRS): coordinate reference system\n",
    "        total_bounds (np.ndarray, optional): If provided, will filter out points that fall\n",
    "            outside of these bounds. Composed of xmin, ymin, xmax, ymax.\n",
    "    \"\"\"\n",
    "    lats = lat.ravel()\n",
    "    lons = lon.ravel()\n",
    "    n_orbits = len(corrected_arr)\n",
    "    size = lats.size\n",
    "    values = {\n",
    "        \"value\": np.concatenate([d.data.ravel() for d in corrected_arr]),\n",
    "        \"lat\": np.tile(lats, n_orbits),\n",
    "        \"lon\": np.tile(lons, n_orbits),\n",
    "        \"orbit\": np.arange(n_orbits).repeat(size),\n",
    "        \"granule_id\": [granule_id] * size * n_orbits\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(values).dropna()\n",
    "    if total_bounds is not None:\n",
    "        x_min, y_min, x_max, y_max = total_bounds\n",
    "        df = df[df.lon.between(x_min, x_max) & df.lat.between(y_min, y_max)]\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(df)\n",
    "    gdf[\"geometry\"] = gpd.points_from_xy(gdf.lon, gdf.lat)\n",
    "    gdf.crs = crs\n",
    "    return gdf[[\"granule_id\", \"orbit\", \"geometry\", \"value\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_calibration_dict(data: SDS):\n",
    "    \"\"\"Define calibration dictionary given a SDS dataset,\n",
    "    which contains:\n",
    "        - name\n",
    "        - scale factor\n",
    "        - offset\n",
    "        - unit\n",
    "        - fill value\n",
    "        - valid range\n",
    "    \n",
    "    Args:\n",
    "        data (SDS): dataset in the SDS format.\n",
    "    \n",
    "    Returns:\n",
    "        calibration_dict (Dict): dict of calibration parameters.\n",
    "    \"\"\"\n",
    "    return data.attributes()\n",
    "\n",
    "\n",
    "def create_alignment_dict(hdf: SD):\n",
    "    \"\"\"Define alignment dictionary given a SD data file, \n",
    "    which contains:\n",
    "        - upper left coordinates\n",
    "        - lower right coordinates\n",
    "        - coordinate reference system (CRS)\n",
    "        - CRS parameters\n",
    "    \n",
    "    Args:\n",
    "        hdf (SD): hdf data object\n",
    "    \n",
    "    Returns:\n",
    "        alignment_dict (Dict): dict of alignment parameters.\n",
    "    \"\"\"\n",
    "    group_1 = hdf.attributes()[\"StructMetadata.0\"].split(\"END_GROUP=GRID_1\")[0]\n",
    "    hdf_metadata = dict([x.split(\"=\") for x in group_1.split() if \"=\" in x])\n",
    "    alignment_dict = {\n",
    "        \"upper_left\": eval(hdf_metadata[\"UpperLeftPointMtrs\"]),\n",
    "        \"lower_right\": eval(hdf_metadata[\"LowerRightMtrs\"]),\n",
    "        \"crs\": hdf_metadata[\"Projection\"],\n",
    "        \"crs_params\": hdf_metadata[\"ProjParams\"]\n",
    "    }\n",
    "    return alignment_dict\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_maiac_data(\n",
    "    granule_path: str,\n",
    "    grid_cell_gdf: gpd.GeoDataFrame,\n",
    "    dataset_name: str,\n",
    "    total_bounds: np.ndarray = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a granule s3 path and competition grid cells, \n",
    "    create a GDF of each intersecting point and the accompanying\n",
    "    dataset value (e.g. blue band AOD).\n",
    "    \n",
    "    Args:\n",
    "        granule_path (str): a path to a granule on s3.\n",
    "        grid_cell_gdf (gpd.GeoDataFrame): GeoDataFrame that contains,\n",
    "            at a minimum, a `grid_id` and `geometry` column of Polygons.\n",
    "        dataset_name (str): specific dataset name (e.g. \"Optical_Depth_047\").\n",
    "        total_bounds (np.ndarray, optional): If provided, will filter out points that fall\n",
    "            outside of these bounds. Composed of xmin, ymin, xmax, ymax.    \n",
    "    Returns:\n",
    "        GeoDataFrame that contains Points and associated values.\n",
    "    \"\"\"\n",
    "    # Load blue band AOD data\n",
    "    # s3_path = S3Path(granule_path, S3Client(no_sign_request=True))\n",
    "    hdf = SD(str(granule_path), SDC.READ)\n",
    "    aod = hdf.select(dataset_name)\n",
    "    shape = aod.info()[2]\n",
    "\n",
    "    # Calibrate and align data\n",
    "    calibration_dict = aod.attributes()\n",
    "    alignment_dict = create_alignment_dict(hdf)\n",
    "    corrected_AOD = calibrate_data(aod, shape, calibration_dict)\n",
    "    xv, yv = create_meshgrid(alignment_dict, shape)\n",
    "    lon, lat = transform_arrays(xv, yv, sinu_crs, wgs84_crs)\n",
    "\n",
    "    # Save values that align with granules\n",
    "    granule_gdf = convert_array_to_df(corrected_AOD, lat, lon, granule_path.name, wgs84_crs, grid_cell_gdf.total_bounds)\n",
    "    df = gpd.sjoin(grid_cell_gdf, granule_gdf, how=\"inner\")\n",
    "    \n",
    "    # Clean up files\n",
    "    # Path(s3_path.fspath).unlink()\n",
    "    hdf.end()\n",
    "    return df.drop(columns=\"index_right\").reset_index()\n",
    "\n",
    "    \n",
    "def preprocess_aod_47(granule_paths, grid_cell_gdf, n_jobs=16):\n",
    "    \"\"\"\n",
    "    Given a set of granule s3 paths and competition grid cells, \n",
    "    parallelizes creation of GDFs containing AOD 0.47 µm values and points.\n",
    "    \n",
    "    Args:\n",
    "        granule_paths (List): list of paths on s3.\n",
    "        grid_cell_gdf (gpd.GeoDataFrame): GeoDataFrame that contains,\n",
    "            at a minimum, a `grid_id` and `geometry` column of Polygons.\n",
    "        n_jobs (int, Optional): The number of parallel processes. Defaults to 2.\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame that contains Points and associated values for all granules.\n",
    "    \"\"\"    \n",
    "    args = ((gp, grid_cell_gdf, \"Optical_Depth_047\") for gp in granule_paths)\n",
    "    \n",
    "    results = pqdm(args, preprocess_maiac_data, n_jobs=n_jobs, argument_type=\"args\")\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 8)\n",
      "Optical_Depth_047\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "Optical_Depth_055\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "AOD_Uncertainty\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "FineModeFraction\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "Column_WV\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "AOD_QA\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "AOD_MODEL\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "Injection_Height\n",
      "    Dimensions: ('Orbits:grid1km', 'YDim:grid1km', 'XDim:grid1km')\n",
      "    Shape: (4, 1200, 1200)\n",
      "cosSZA\n",
      "    Dimensions: ('Orbits:grid5km', 'YDim:grid5km', 'XDim:grid5km')\n",
      "    Shape: (4, 240, 240)\n",
      "cosVZA\n",
      "    Dimensions: ('Orbits:grid5km', 'YDim:grid5km', 'XDim:grid5km')\n",
      "    Shape: (4, 240, 240)\n",
      "RelAZ\n",
      "    Dimensions: ('Orbits:grid5km', 'YDim:grid5km', 'XDim:grid5km')\n",
      "    Shape: (4, 240, 240)\n",
      "Scattering_Angle\n",
      "    Dimensions: ('Orbits:grid5km', 'YDim:grid5km', 'XDim:grid5km')\n",
      "    Shape: (4, 240, 240)\n",
      "Glint_Angle\n",
      "    Dimensions: ('Orbits:grid5km', 'YDim:grid5km', 'XDim:grid5km')\n",
      "    Shape: (4, 240, 240)\n",
      "Dataset name: Optical_Depth_047\n",
      "Number of dimensions: 3\n",
      "Shape: [4, 1200, 1200]\n",
      "Data type: 22\n",
      "Number of attributes: 6\n",
      "{'long_name': 'AOD at 0.47 micron', 'scale_factor': 0.001, 'add_offset': 0.0, 'unit': 'none', '_FillValue': -28672, 'valid_range': [-100, 5000]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "grid_id\n",
       "-9209984482513666380    0.122083\n",
       "-9170713913398490059    0.083295\n",
       "-9149710595722273778    0.067205\n",
       "-9136909972871109155    0.110354\n",
       "-9134378635282384679    0.124435\n",
       "                          ...   \n",
       " 9082022442658484871    0.089872\n",
       " 9085670462040955654    0.064207\n",
       " 9096064789242280470    0.084000\n",
       " 9164369667763217457    0.095200\n",
       " 9217372132709696705    0.077207\n",
       "Name: value, Length: 551, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reg_code, reg_name = (\"la\", \"Los Angeles (SoCAB)\")\n",
    "split = 'train'\n",
    "\n",
    "maiac_md = pm_md[(pm_md[\"product\"] == \"maiac\") & (pm_md[\"split\"] == split)].copy()\n",
    "la_file = maiac_md[maiac_md.location == reg_code].iloc[0]\n",
    "la_file_path = S3Path(la_file.us_url, S3Client(no_sign_request=True))\n",
    "\n",
    "hdf = SD(la_file_path.fspath, SDC.READ)\n",
    "print(hdf.info())\n",
    "for dataset, metadata in hdf.datasets().items():\n",
    "    dimensions, shape, _, _ = metadata\n",
    "    print(f\"{dataset}\\n    Dimensions: {dimensions}\\n    Shape: {shape}\")\n",
    "    \n",
    "blue_band_AOD = hdf.select(\"Optical_Depth_047\")\n",
    "\n",
    "name, num_dim, shape, types, num_attr = blue_band_AOD.info()\n",
    "\n",
    "print(\n",
    "f\"\"\"Dataset name: {name}\n",
    "Number of dimensions: {num_dim}\n",
    "Shape: {shape}\n",
    "Data type: {types}\n",
    "Number of attributes: {num_attr}\"\"\"\n",
    ")\n",
    "calibration_dict = blue_band_AOD.attributes()\n",
    "print(calibration_dict)\n",
    "raw_attr = hdf.attributes()[\"StructMetadata.0\"]\n",
    "\n",
    "group_1 = raw_attr.split(\"END_GROUP=GRID_1\")[0]\n",
    "hdf_metadata = dict([x.split(\"=\") for x in group_1.split() if \"=\" in x])\n",
    "\n",
    "# Parse expressions still wrapped in apostrophes\n",
    "for key, val in hdf_metadata.items():\n",
    "    try:\n",
    "        hdf_metadata[key] = eval(val)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "alignment_dict = {\n",
    "    \"upper_left\": hdf_metadata[\"UpperLeftPointMtrs\"],\n",
    "    \"lower_right\": hdf_metadata[\"LowerRightMtrs\"],\n",
    "    \"crs\": hdf_metadata[\"Projection\"],\n",
    "    \"crs_params\": hdf_metadata[\"ProjParams\"]\n",
    "}\n",
    "\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Loop over orbits to apply the attributes\n",
    "\n",
    "corrected_AOD = calibrate_data(blue_band_AOD, shape, calibration_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xv, yv = create_meshgrid(alignment_dict, shape)\n",
    "\n",
    "\n",
    "\n",
    "# Source: https://spatialreference.org/ref/sr-org/modis-sinusoidal/proj4js/\n",
    "sinu_crs = Proj(f\"+proj=sinu +R={alignment_dict['crs_params'][0]} +nadgrids=@null +wktext\").crs\n",
    "wgs84_crs = CRS.from_epsg(\"4326\")\n",
    "\n",
    "# Project sinu grid onto wgs84 grid\n",
    "lon, lat = transform_arrays(xv, yv, sinu_crs, wgs84_crs)\n",
    "\n",
    "\n",
    "gdf = convert_array_to_df(corrected_AOD, lat, lon, la_file_path.stem, wgs84_crs)\n",
    "\n",
    "# Identify LA granule filepaths (2019) and grid cells\n",
    "maiac_md = maiac_md[maiac_md.location == reg_code]\n",
    "\n",
    "la_fp = sorted(list(Path('/mnt/d/airdata/raw/train/maiac').glob(r'**/*.hdf')))\n",
    "\n",
    "la_gc = grid_md[grid_md.location == reg_name].copy()\n",
    "\n",
    "# Load training labels\n",
    "# train_labels = pd.read_csv(DATA_PATH / \"train_labels.csv\", parse_dates=[\"datetime\"])\n",
    "# train_labels.rename(columns={\"value\": \"pm25\"}, inplace=True)\n",
    "la_polys = gpd.GeoSeries.from_wkt(la_gc.wkt, crs=wgs84_crs) # used for WGS 84\n",
    "la_polys.name = \"geometry\"\n",
    "la_polys_gdf = gpd.GeoDataFrame(la_polys)\n",
    "la_polys_gdf['grid_id'] = la_polys_gdf['geometry'].apply(lambda x: hash(str(x))) \n",
    "xmin, ymin, xmax, ymax = la_polys_gdf.total_bounds\n",
    "gpd.sjoin(la_polys_gdf, gdf.cx[xmin:xmax, ymin:ymax], how=\"inner\").groupby(\"grid_id\")[\"value\"].mean()\n",
    "\n",
    "# la_train_gdf = preprocess_aod_47(la_fp, la_polys_gdf)\n",
    "# # la_train_gdf.to_file(f\"{reg_code}_{split}_{i}.shp\")\n",
    "# full_data = calculate_features(la_train_gdf)\n",
    "# full_data.to_csv(f\"{reg_code}_{split}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,p2,p3,p4,p5 = np.array_split(la_polys_gdf, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la_train_gdf = preprocess_aod_47(la_fp, p1)\n",
    "# # # la_train_gdf.to_file(f\"{reg_code}_{split}_{i}.shp\")\n",
    "# full_data = calculate_features(la_train_gdf)\n",
    "# full_data.to_csv(f\"{reg_code}_{split}_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la_train_gdf = preprocess_aod_47(la_fp, p2)\n",
    "# # # la_train_gdf.to_file(f\"{reg_code}_{split}_{i}.shp\")\n",
    "# full_data = calculate_features(la_train_gdf)\n",
    "# full_data.to_csv(f\"{reg_code}_{split}_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la_train_gdf = preprocess_aod_47(la_fp, p3)\n",
    "# # # la_train_gdf.to_file(f\"{reg_code}_{split}_{i}.shp\")\n",
    "# full_data = calculate_features(la_train_gdf)\n",
    "# full_data.to_csv(f\"{reg_code}_{split}_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la_train_gdf = preprocess_aod_47(la_fp, p4)\n",
    "# # # la_train_gdf.to_file(f  \"{reg_code}_{split}_{i}.shp\")\n",
    "# full_data = calculate_features(la_train_gdf)\n",
    "# full_data.to_csv(f\"{reg_code}_{split}_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 4260it [00:00, 7830.94it/s] \n",
      "PROCESSING TASKS | : 100%|██████████| 4260/4260 [13:02<00:00,  5.45it/s]\n",
      "COLLECTING RESULTS | : 100%|██████████| 4260/4260 [00:00<00:00, 896479.61it/s]\n",
      "/mnt/d/weather-forecast/.venv/lib/python3.8/site-packages/geopandas/geodataframe.py:1350: UserWarning: Geometry column does not contain geometry.\n",
      "  warnings.warn(\"Geometry column does not contain geometry.\")\n"
     ]
    }
   ],
   "source": [
    "la_train_gdf = preprocess_aod_47(la_fp, p5)\n",
    "# # la_train_gdf.to_file(f\"{reg_code}_{split}_{i}.shp\")\n",
    "full_data = calculate_features(la_train_gdf)\n",
    "full_data.to_csv(f\"{reg_code}_{split}_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [f'la_train_{i}.csv' for i in range(1, 6)]\n",
    "dfs = [pd.read_csv(i) for i in files]\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df.to_csv(\"la_train_grid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555    305\n",
       "554     29\n",
       "553     22\n",
       "525     20\n",
       "551     18\n",
       "      ... \n",
       "54       1\n",
       "442      1\n",
       "429      1\n",
       "191      1\n",
       "283      1\n",
       "Name: geometry, Length: 278, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('day').count()['geometry'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cbcc1d2ab9eeb7f091bf07098d66cd91656ee362ace8b19d304bcdedf32b4b19"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
